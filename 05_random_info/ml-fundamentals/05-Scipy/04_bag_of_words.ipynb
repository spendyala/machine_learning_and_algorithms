{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Vectorize tweets using Bag of Words\n",
    "\n",
    "Bag of words vectorization uses each possible word (or token) as a feature that is present or absent. Each feature value can either be the number of occurrences of a token for an instance or simply denote the presence or absence of a token with a 1 or a 0.\n",
    "\n",
    "This creates a very large feature space where every possible word is a feature.\n",
    "\n",
    "Each tweet, especially because of twitter size limits, will have a small number of words.\n",
    "\n",
    "For example, if the vocabulary of words is 100,000 tokens and each tweet on average is 8 words, then the bag of words representation for each tweet will have around 99,992 zeros. This will require a lot of memory storage, especially as the number of tweets (and/or the vocabulary) grows.\n",
    "\n",
    "### Reducing the feature space\n",
    "\n",
    "There are techniques for reducing the number of tokens in the vocabulary and hence the number of features, including:\n",
    "\n",
    "* Replacing new or low-frequency terms with the \"unk\" (unknown) token\n",
    "* Pruning out \"stop words\" or words that are so common that they don't convey meaning (like \"the\", and \"and\".)\n",
    "   * Note that these would likely depend on each problem and its semantic scope.\n",
    "* Normalizing tokens (e.g., common-casing, stripping punctuation) to reduce duplication\n",
    "* Collapsing categories of tokens into a single token representing the full category (for example all numbers are represented by the \"number\" token.\n",
    "\n",
    "Application of any of these or other techniques depends on the problem being solved.\n",
    "\n",
    "### Reducing the storage requirements\n",
    "\n",
    "We can also use effecient data structures for storing relevant data instead of carrying around all those zeros. And, thanks to scipy's sparse package, we can do this without also losing all of the matrix math provided by numpy arrays!\n",
    "\n",
    "## Case study: Tweet vectorization\n",
    "\n",
    "Let's walk through the concrete example of vectorizing tweets for the purpose of predicting emojis relevant to each tweet using bag of words.\n",
    "\n",
    "First, some initializations including reading in our sample tweets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy\n",
    "import sys\n",
    "from collections import Counter\n",
    "from scipy.sparse import coo_matrix, csc_matrix, csr_matrix, eye, hstack, linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download the tweet data\n",
    "mkdir -p ../.data\n",
    "cd ../.data\n",
    "if [ ! -f emoji_tweets_5k.csv ]; then\n",
    "    echo \"File not found. Downloading from s3\"\n",
    "    wget -q https://s3-us-west-2.amazonaws.com/resero2/datasets/ml-foundations/emoji_tweets_5k.csv\n",
    "else\n",
    "    echo \"File exists, not downloading from s3\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tweet data\n",
    "import csv\n",
    "import json\n",
    "\n",
    "texts = []\n",
    "emojis = []\n",
    "\n",
    "with open(\"../.data/emoji_tweets_5k.csv\") as infile:\n",
    "    for row in csv.reader(infile):\n",
    "        text = json.loads(row[1]).strip()\n",
    "        texts.append(text)\n",
    "        emojis.append(json.loads(row[2]))\n",
    "\n",
    "print(f'Text count: {len(texts)}')\n",
    "print(f'Emojis count: {len(emojis)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To recall what these data look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's tokenize tweets into words such that we:\n",
    "\n",
    "* Collapse all special twitter tokens:\n",
    "   * user\n",
    "   * hashtag\n",
    "   * ticker\n",
    "   * signed\n",
    "* Collapse urls and emails\n",
    "* Collapse tokens having a frequency of 5 or below (including new tokens) as \"unknown\"\n",
    "* Normalize token case\n",
    "* Remove trailing punctuation and empty tokens\n",
    "\n",
    "To spare you the gory details, I created a tokenizer class that does all of this.\n",
    "\n",
    "So let's test this a little bit and build our feature matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.token_model as token_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import importlib; importlib.reload(token_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = token_model.TokenModel()\n",
    "tokenizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a peek at what to expect when collapsing tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.info(token_model.TokenModel.collapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(tokenizer.tokenize('.@mention1 @mention2 #hashtag $ticker me@foo.com http://bit.ly wow. ^signed jk.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts[0:5]:\n",
    "    tokenized = ' '.join(tokenizer.tokenize(text))\n",
    "    print(f'\\t{text}\\n\\t{tokenized}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the token frequencies...\n",
    "\n",
    "* Count the number of occurrences of each token in the data set\n",
    "* Lay the tokens down in order from most to least frequent\n",
    "* The first token will be the most frequent and will have rank 0\n",
    "   * Note that those tokens with the same count will be adjacent, but arbitrarily ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot token frequencies ranked from highest to lowest\n",
    "def show_freqs(start_freq=0):\n",
    "    _, axis = plt.subplots()\n",
    "    freqs = np.array([value for _,value in tokenizer.tok2freq.most_common()], dtype=np.int32)\n",
    "    plt.plot(range(start_freq, len(freqs)), freqs[start_freq:])\n",
    "    plt.xlabel('token rank (ordered most to least frequent)')\n",
    "    plt.ylabel('token frequency (count of tokens)')\n",
    "    maxfreq = freqs[start_freq]\n",
    "    cutoff = np.where(freqs <= 5)[0][0]\n",
    "    axis.annotate(f'freq<=5 (rank={cutoff}, token=\"{tokenizer.tok2freq.most_common()[cutoff][0]}\"\")',\n",
    "                  xy=(cutoff, freqs[cutoff]),\n",
    "                  xytext=(cutoff+200, freqs[cutoff]+maxfreq/6),\n",
    "                  arrowprops=dict(facecolor='black', shrink=0.05, width=2))\n",
    "show_freqs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_freqs(start_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the most frequent tokens in the dataset (histogram):\n",
    "tokenizer.tok2freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the least frequent tokens in the dataset (histogram):\n",
    "tokenizer.tok2freq.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With tokenization worked out, it's time to vectorize all of the tweets...\n",
    "\n",
    "* Create a sparse COO matrix where\n",
    "   * each row represents a tweet\n",
    "   * each column in each row is a bag of words vector encoding the count of each token present in the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's vectorize just one to get a feel for what's going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a tweet\n",
    "tweet_idx = 0\n",
    "\n",
    "print(texts[tweet_idx])\n",
    "print(' '.join(tokenizer.tokenize(texts[tweet_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize this tweet\n",
    "tweet_tokens = tokenizer.transform(texts[tweet_idx]).toarray()\n",
    "tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(tweet_tokens > 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse lookup the tokens that were set\n",
    "nonzero = np.where(tweet_tokens > 0)\n",
    "counts = tweet_tokens[nonzero]\n",
    "ranks = nonzero[1]\n",
    "tokens = [tokenizer.idx2tok(rank) for rank in ranks]\n",
    "for token, count, rank in zip(tokens, counts, ranks):\n",
    "    print(f'\\t{token}\\tcount={count}\\trank={rank}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's vectorize them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "X = tokenizer.transform(texts)\n",
    "\n",
    "# Plot the \"spy\" chart of the resulting feature matrix\n",
    "plt.spy(X.toarray(), aspect='auto')\n",
    "plt.show()\n",
    "print(f'X.shape = {X.shape}, meaning we have {X.shape[0]} tweets, each having {X.shape[1]} features')\n",
    "elt_count = X.shape[0] * X.shape[1]\n",
    "sparse_size = X.data.nbytes + X.indptr.nbytes + X.indices.nbytes\n",
    "dense_size = 8 * elt_count  # to store int8's\n",
    "print(f'sparse size = {sparse_size} B, dense size = {dense_size} B')\n",
    "density = X.count_nonzero() / elt_count\n",
    "print(f'density = {density}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we're counting token occurrences within tweets, let's see how many repetitions occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find non-zero values and the counts of each value as\n",
    "#    value: count\n",
    "# Where value is the number of times a token appeared in a tweet\n",
    "# and count is the number of features occuring \"value\" times\n",
    "nzr, nzc, nzv, vcounts = tokenizer.find_nonzero(X)\n",
    "tokenizer.show_vcounts(vcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map these values back to the data to see:\n",
    "#  - the repeating token\n",
    "#  - the number of repeats\n",
    "#  - the original tweet text\n",
    "#  - the tokenized tweet text\n",
    "# For certain repeat counts.\n",
    "list(map(\n",
    "    lambda idx: (tokenizer.idx2tok(nzc[idx]),\n",
    "                 nzv[idx],\n",
    "                 texts[nzr[idx]],\n",
    "                 ' '.join(tokenizer.tokenize(texts[nzr[idx]]))),\n",
    "    np.where(nzv > 17)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the most common repeating tokens *within a tweet* (histogram):\n",
    "rtokens = Counter()\n",
    "for tok, count in map(lambda idx: (tokenizer.idx2tok(nzc[idx]), nzv[idx]), np.where(nzv >= 5)[0]):\n",
    "    rtokens[tok] += count\n",
    "rtokens.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the tokens that repeat the most within tweets are similar to the most common tokens overall.\n",
    "Recall that the UNK token represents all of the \"long tail\" tokens with frequency < 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's organize the emojis for modeling and create the train/test split...\n",
    "\n",
    "* As previously done, we'll focus on just the top 10 emojis for targets\n",
    "* And we'll split the data 90% for training, leaving 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_counter = Counter()\n",
    "for emoji_dict in emojis:\n",
    "    emoji_counter.update(emoji_dict.keys())\n",
    "\n",
    "print(emoji_counter.most_common(10))\n",
    "\n",
    "common_emojis = [e[0] for e in emoji_counter.most_common(10)]\n",
    "\n",
    "print()\n",
    "print('Top 10:')\n",
    "print(common_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two useful lookup tables for converting emojis into integer indexes and vice-versa\n",
    "emoji_to_index = {w : i for i, w in enumerate(common_emojis)}\n",
    "index_to_emoji = {i : w for i, w in enumerate(common_emojis)}\n",
    "\n",
    "def create_Y_matrix(emojis):\n",
    "    n = len(common_emojis)\n",
    "    m = len(emojis)\n",
    "    Y = np.zeros([m, n])\n",
    "    for i, single_tweet_emojis in enumerate(emojis):\n",
    "        for emoji in single_tweet_emojis:\n",
    "            index = emoji_to_index.get(emoji, None)\n",
    "            if index is not None:\n",
    "                Y[i, index] = 1\n",
    "    return Y\n",
    "                \n",
    "Y = create_Y_matrix(emojis)\n",
    "\n",
    "print(f'Y shape: {Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "num_train = int(N * 0.9)\n",
    "\n",
    "# Convert values to 1's for training\n",
    "X1 = csr_matrix((np.ones(X.data.shape[0], dtype=np.int8), X.indices, X.indptr))\n",
    "\n",
    "def split(array, split_point=num_train):\n",
    "    return array[:split_point], array[split_point:]\n",
    "\n",
    "X_train, X_test = split(X1)\n",
    "Y_train, Y_test = split(Y)\n",
    "texts_train, texts_test = split(texts)\n",
    "emojis_train, emojis_test = split(emojis)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print()\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "print(f'Y_test shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we're ready to build a model!\n",
    "\n",
    "* This time, we're building a model based on *sparse matrices* instead of *numpy arrays*\n",
    "   * see the ^^^ attention marker in the code comments\n",
    "* Just as we did in lesson #1, we'll use multivariate least squares regression:\n",
    "\n",
    "\n",
    "Given the encoded emoji matrix, $\\mathbf{y}$, and our featurized input matrix, $\\mathbf{X}$ (prepended with a 1 for each instance to be multipled by the bias in the $\\mathbf{\\beta}$ vector), we're solving for $\\mathbf{\\beta}$:\n",
    "\n",
    "$$ \\mathbf{X} \\mathbf{\\beta} = \\mathbf{y} $$\n",
    "\n",
    "$$ \\quad \\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & ... & X_{1,n} \\\\ 1 & X_{2,1} & X_{2,2} & ... & X_{2,n} \\\\ ... & ... & ... & ... & ... \\\\ 1 & X_{m,1} & X_{m,2} & ... & X_{m,n} \\end{bmatrix} \\begin{bmatrix} b_{1} & b_{2} & ... & b_{t} \\\\ w_{1,1} & w_{1,2} & ... & w_{1,t} \\\\ w_{2,1} & w_{2,2} & ... & w_{2,t} \\\\ ... & ... & ... & ... \\\\ w_{n,1} & w_{n,2} & ... & w_{n,t} \\end{bmatrix} = \\begin{bmatrix} y_{1,1} & y_{1,2} & ... & y_{1,t} \\\\ y_{2,1} & y_{2,2} & ... & y_{2,t} \\\\ ... & ... & ... & ... \\\\ y_{m,1} & y_{m,2} & ... & y_{m,t} \\end{bmatrix} $$\n",
    "\n",
    "Recall that solving for $\\mathbf{\\beta}$ and regularizing using \"Ridge Regression\", we get\n",
    "\n",
    "$$ \\mathbf{\\beta} = ( \\mathbf{X}^T \\mathbf{X} + \\lambda I )^{-1} \\mathbf{X}^T \\mathbf{y} $$\n",
    "\n",
    "where $I$ is the identity matrix and $\\lambda$ is the \"ridge value\", a small number, e.g., around $10^{-1}$ or $10^{-3}$.\n",
    "\n",
    "It turns out that we need to use the ridge regression because the sparse values in our matrix ($\\mathbf{X}^T$ $\\mathbf{X}$) is singular (uninvertible). The regularization along the diagonal ensures that the determinant is non-zero and, therefore, invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a couple of helpers to keep things straight\n",
    "\n",
    "def augment_X(X):\n",
    "    '''\n",
    "    Augment sparse X with the Bias vector (Beta) column of ones.\n",
    "    We'll need to do this when training and when predicting.\n",
    "    '''\n",
    "    ones_col = csc_matrix(np.ones([X.shape[0], 1], dtype=np.float32))  # ^^^ Wrap np.ones with a sparse column matrix\n",
    "    return hstack([ones_col, X], format='csr', dtype=np.float32)  # ^^^ Contrast sparse.hstack with np.hstack([ones_col, X])\n",
    "\n",
    "class DataWrapper:\n",
    "    '''\n",
    "    Simple wrapper for adding the Beta column to X, holding y, and remembering sizes\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: a 2d ndarray with shape (m, n) holding the independent variables\n",
    "        :param y: a 2d ndarray with shape (m, 1) holding the targets\n",
    "        \"\"\"\n",
    "        self.m, self.n = X.shape\n",
    "        assert y.shape[0] == self.m\n",
    "        \n",
    "        # Augment X with the ones column\n",
    "        self.X = augment_X(X)\n",
    "        self.y = csr_matrix(y, dtype=np.float32)   # ^^^ Wrap y with a sparse row matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = DataWrapper(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For reference, here is the least_squares implementation from lesson #1\n",
    "# Dense (numpy) implementation\n",
    "\n",
    "def least_squares(X, y):\n",
    "    \"\"\"\n",
    "    :param X: a 2d ndarray with shape (m, n) holding the independent variables\n",
    "    :param y: a 2d ndarray with shape (m, 1) holding the targets\n",
    "    :returns: a 2d ndarray with shape (n+1, 1) holding the bias (first element) and the weights (rest of the elements)\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert y.shape[0] == m\n",
    "    \n",
    "    # Augment X\n",
    "    ones_col = np.ones([m, 1])\n",
    "    X = np.hstack([ones_col, X])\n",
    "\n",
    "    # Solve the normal equations\n",
    "    result_one = np.linalg.inv(np.matmul(X.T, X))\n",
    "    result_two = np.matmul(result_one, X.T)\n",
    "    return np.matmul(result_two, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse implementation with ridge regression\n",
    "def least_squares(dw, l=1.0e-2):\n",
    "    \"\"\"\n",
    "    :param dw: a DataWrapper holding X and y\n",
    "    :param l: Lambda for ridge regression, if present\n",
    "    :returns: a 2d sparse array with shape (n+1, 1) holding the bias (first element) and the weights (rest of the elements)\n",
    "    \"\"\"\n",
    "    # Solve the normal equations\n",
    "    Xt = dw.X.T\n",
    "    result_one = Xt @ dw.X  # ^^^ Contrast with non-sparse np.matmul(Xt, dw.X)\n",
    "    \n",
    "    # Regularize for inverting (ridge regularization)\n",
    "    if l is not None:\n",
    "        diag = np.ones(dw.n+1, dtype=np.float32) * l\n",
    "        diag[0] = 0  # don't regularize the bias term\n",
    "        I = eye(dw.n+1, dw.n+1, dtype=np.float32, format='csr')  # sparse \"eye\"-dentity matrix, get it?!\n",
    "        I.setdiag(diag)\n",
    "        result_one = result_one + I  # ^^^ sparse + sparse = sparse\n",
    "\n",
    "    # Carry on with the calculations\n",
    "    result_two = linalg.inv(result_one)  # ^^^ Contrast sparse.linalg.inv with non-sparse np.linalg.inv\n",
    "    result_three = result_two @ Xt  # ^^^ Contrast with non-sparse np.matmul(result_two, Xt)\n",
    "    return result_three @ dw.y  # ^^^ Contrast with non-sparse np.matmul(result_three, dw.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And ... solve the model!\n",
    "Betas = least_squares(dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look, ma, we're still working with sparse matrices!\n",
    "Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the biases and the weights\n",
    "Biases = Betas[0, :]\n",
    "Weights = Betas[1:, :]\n",
    "\n",
    "print(f'Betas shape: {Betas.shape}')\n",
    "print(f'Biases shape: {Biases.shape}')\n",
    "print(f'Weights shape: {Weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and the slices are also still sparse...\n",
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Betas):\n",
    "    \"\"\"\n",
    "    :param X: a 2d ndarray with shape (m, n) holding the independent variables\n",
    "    :param Betas: a 2d ndarray with shape (n+1, k) holding the parameters of a linear model (the first row contains bias terms)\n",
    "    :returns: a 2d ndarray with shape (m, k) holding the predictions\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert Betas.shape[0] == n + 1\n",
    "    \n",
    "    # Augment X with the ones column\n",
    "    X = augment_X(X)\n",
    "\n",
    "    # Apply model\n",
    "    return X @ Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = predict(X_test, Betas)\n",
    "print(f'Y_test_pred shape: {Y_test_pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Print results when an emoji prediction score exceeds a threshold (0.4)\n",
    "#\n",
    "result_counts = Counter()\n",
    "for test_text, test_emoji, y_pred in zip(texts_test, emojis_test, Y_test_pred):\n",
    "    y_pred = y_pred.toarray()[0]\n",
    "    highest_scoring_emoji_index = y_pred.argmax()  # ^^^ Contrast with non-sparse np.argmax(y_pred)\n",
    "    highest_score = y_pred[highest_scoring_emoji_index]\n",
    "    tweets_common_emojis = [e for e in test_emoji if e in common_emojis]\n",
    "    if len(tweets_common_emojis) > 0:\n",
    "        result_counts['instances_with_emoji'] += 1\n",
    "    if highest_score > 0.4:\n",
    "        print('-'*40)\n",
    "        print('Text:', test_text)\n",
    "        print('Bag of words:', ' '.join(tokenizer.tokenize(test_text)))\n",
    "        tweets_common_emojis = [e for e in test_emoji if e in common_emojis]\n",
    "        print('Common emojis:', tweets_common_emojis)\n",
    "        ordered_preds = sorted(zip(y_pred, common_emojis), reverse=True)\n",
    "        if ordered_preds[0][1] in tweets_common_emojis:\n",
    "            result_counts['top_match'] += 1\n",
    "        for pred, emoji in ordered_preds:\n",
    "            if emoji in tweets_common_emojis:\n",
    "                result_counts['any_match'] += 1\n",
    "                break\n",
    "        print(ordered_preds)\n",
    "print(f'\\nresult_counts: {result_counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare above results to lesson 1:\n",
    "   * result_counts: Counter({'instances_with_emoji': 214, 'any_match': 17, 'top_match': 14})\n",
    "   * We doubled the number of top matches!\n",
    "* Differences:\n",
    "   * Used sparse matrices (which shouldn't make any difference in the results)\n",
    "   * Evolved tokenization for the bag of words features\n",
    "   * Added regularization\n",
    "* Next steps:\n",
    "   * Consider more feature engineering\n",
    "   * Need a lot more training examples\n",
    "      * And this is where the sparse matrices will be more necessary\n",
    "   * Try other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show words that are important (have the highest absolute weights) for an emoji\n",
    "def print_important_words(emoji, count=10):\n",
    "    emoji_index = emoji_to_index[emoji]\n",
    "    emoji_betas = Betas[:, emoji_index]\n",
    "    emoji_word_weights = emoji_betas[1:].toarray().ravel() # first term is bias\n",
    "    sorted_idxs = np.argsort(np.abs(emoji_word_weights))[::-1]\n",
    "    for idx in sorted_idxs[:count]:\n",
    "        print(f'{idx}\\t{emoji_word_weights[idx]}\\t{tokenizer.idx2tok(idx)}')\n",
    "\n",
    "print_important_words('❤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_important_words('🔥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
