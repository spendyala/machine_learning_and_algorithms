{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Basics\n",
    "===================================\n",
    "\n",
    "In our overview we demonstrated how we could use pandas as a tool to help us to better understand the data that we are \n",
    "working with.  For me this is the biggest benefit that Pandas provides for use as Data Scientist, a tool to quickly\n",
    "explore our dataset and through exploration to better understand the data.  \n",
    "\n",
    "Now, I will start by saying that Pandas is not a silver bullet (as with almost all technologies).  Pandas, by itself, is used\n",
    "by loading all the data into memory, which means on larger datasets Pandas starts to fail with OOM errors and incredible\n",
    "long execution times.  However, if you can work with just a subset of the larger dataset it can still give you some very powerful\n",
    "insight into your data.  \n",
    "\n",
    "For this notebook I am going to focus more on having you guys learn through practice then by giving you concrete examples, to\n",
    "that end I hope that you can follow along and gain a greater appreciation for pandas and the power that it provides.  \n",
    "\n",
    "To really understand and appreciate pandas we are going to need to use it with a dataset. For this notebook we are going to \n",
    "use the [City of Austin - Traffic Count Study](https://catalog.data.gov/dataset/traffic-count-study-area/resource/820bc731-bc7c-4598-a08d-8430ad141c60).\n",
    "\n",
    "This dataset contains the following details.  \n",
    "\n",
    "  * Location (`24 HOUR VOLUME COUNT LOCATIONS`)\n",
    "  * Northbound Total (`NB TOTAL`)\n",
    "  * Southbound Total (`SB TOTAL`) \n",
    "  * Eastbound Total (`EB TOTAL`)\n",
    "  * Westbound Total (`WB TOTOAL`)\n",
    "  * Total Volumen (`TOTAL VOLUME`)\n",
    "  * Measurement Date (`DATE`)\n",
    "    \n",
    "To start, we need to first import our needed modules and download the dataset we are going to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from ml_course.util.downloader import download_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.austintexas.gov/api/views/cqdh-farx/rows.csv?accessType=DOWNLOAD'\n",
    "save_name = 'austin.csv'\n",
    "austin_filename = download_data(url, save_name)\n",
    "\n",
    "!head -n 5 /home/jovyan/project/ml_course/util/../../.data/austin.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "While pandas does have a lot of functionality, there are really just two main building blocks that most everything\n",
    "else is built upon.  We are going to start out discussion focused on these two building blocks, how they interact\n",
    "and common ways to work with them.  \n",
    "\n",
    "The two main building blocks are the __DataFrame__ and __Series__ types.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "So, what is a dataframe?  To put it in simple terms, `a dataframe is like a relational database table`.  This is\n",
    "by far the easist way to see it from a Software Engineer's perspective.  The more formal answer comes from the\n",
    "original source that a dataframe was modeled from, which is a data frame in the __R__ programming language.  \n",
    "\n",
    "> A data.frame object in R has similar dimensional properties to a matrix, but it may contain categorical data\n",
    "as well as numeric. Each column in the data.frame is a vector containing the variable value for a given data instance,\n",
    "with each row corresponding to that instance.  \n",
    "\n",
    "Now a Pandas DataFrame consists of three main parts, these are:\n",
    "\n",
    "- The data\n",
    "- The index (label)\n",
    "- The columns (label)\n",
    "\n",
    "Now, some of you may be asking what is the difference between a 2 dimensional numpy array and a pandas DataFrame?  There are a couple\n",
    "of key things.  \n",
    "\n",
    "1. Two-dimensional numpy arrays must all be the same type but pandas columns can be different types\n",
    "2. Rows and Columns can be queried using custom string labels, not just offset indexes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Dataframe\n",
    "\n",
    "So let's start by creating our first dataframe and seeing what it provides for us.  There are several ways that we can\n",
    "create a dataframe, but for this notebook we are only going to focus on one version for simplicity. \n",
    "\n",
    "**NOTE:** For more details please look at the [pandas.DataFrame documentation here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {'id': 1, 'name': 'Frank', 'email': 'pandas@gmail.com'},\n",
    "    {'id': 2, 'name': 'Ryan', 'email': 'asure@yahoo.com'},\n",
    "    {'id': 3, 'name': 'George', 'email': 'old@aol.com'},\n",
    "    {'id': 4, 'name': 'Jack', 'email': 'movingto@outlook.com'},\n",
    "    {'id': 5, 'name': 'Luke', 'email': 'longtimeago@oldrepublic.com'}\n",
    "], index=['A', 'B', 'C', 'D', 'E'])\n",
    "display(df)\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output above, there are a few things that we can immediately see.  \n",
    "\n",
    "1. The output looks like a table with two axes. \n",
    "2. Our axis values have string labels, not just numeric offsets\n",
    "  - Rows: A, B, C, D, E\n",
    "  - Columns: email, id, name\n",
    "3. Our dataframe consists of multiple types:\n",
    "  - object: email, name\n",
    "  - int64: id\n",
    "\n",
    "From the datatypes describe above, we see that there is an `int64`.  This datatype is part of the numpy dtype.\n",
    "Pandas is built on numpy and as such uses numpy dtypes for numeric columns.  The other type `object` is a little\n",
    "bit different.  As we mentioned above, a dataframe can store not just numeric data, but also categorical data in\n",
    "the form of strings.  Since strings don't have a numeric representation, they fall into the numpy catchall of\n",
    "`object`.  \n",
    "\n",
    "What is really interesting about the dtypes is that we didn't explicitly specify the type in the dataframe creation,\n",
    "rather the dataframe took the time to __guess__ what the correct datatype should be.  Later on we will go over how\n",
    "to fix the type when it guesses incorrectly but for now we can work with the types that have been supplied.  \n",
    "\n",
    "Dataframes also have some very handy functions that we can use on them to view the data, some of these methods are:\n",
    "\n",
    "- `head(n)` - Displays the first `n` rows from the dataframe\n",
    "- `tail(n)` - Like head, but displays the last `n` rows instead\n",
    "- `info(n)` - Overview of the dataframe including dtypes, shape and other info\n",
    "\n",
    "Let's practice with these methods on the austin dataset.  Since we have not yet created a dataframe from that\n",
    "dataset, we will first load the csv into a dataframe before running our first exercise.  \n",
    "\n",
    "**NOTE:** While we aren't really covering it here, pandas does provide a lot of functions for working with other\n",
    "data sources including: `excel`, `json`, `html` and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I will use df for most of the time a dataframe is created as a simple shorthand\n",
    "df = pd.read_csv(austin_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "# 1. Using head, determine what the EB TOTAL is for the row with the index of 4\n",
    "\n",
    "# 2. Using tail, determine what the SB TOTAL is for the row with the index of 3331\n",
    "\n",
    "# 3. Using info, say what the data type is for NB TOTAL and EB TOTAL\n",
    "\n",
    "# 4. Using info, determine how many entries there are in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to use head/tail and info are useful tools in giving us a quick view at our data.  Now that we can see some\n",
    "values, we may want to retrieve specific values so that we can use the results in operations.  Because our axes have labels\n",
    "we can select data using the supplied labels instead of the exact indexes (as you would have to in numpy).  \n",
    "\n",
    "The mechanism that we will use, which is one of many, is the `loc` method.  You can find more documentation for this method\n",
    "[here](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html#pandas.DataFrame.loc).  The basic\n",
    "syntax for a dataframe loc method is below:\n",
    "\n",
    "        df.loc[row, column]\n",
    "        \n",
    "Lets try this out by retrieving the values that we found above using the `loc` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "\n",
    "# Using loc get the value for the EB TOTAL column with a row of index 4\n",
    "result = df.loc[_, _]\n",
    "assert result == 373.0\n",
    "\n",
    "# Using the loc get the value for the SB TOTAL column with a row of index 3331\n",
    "result = df.loc[_, _]\n",
    "assert result == 903.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loc` method can also work with more than just a single row and single column, namely each label can be on of:\n",
    "\n",
    "- A single label (as in exercise 2)\n",
    "- A list of array of labels\n",
    "- A slice object with labels\n",
    "- A boolean array\n",
    "\n",
    "Lets try out a few of these different versions to see what information we get from each.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "\n",
    "# Using loc create a window dataframe for rows 1000 to 1005 for the columns NB TOTAL and SB TOTAL\n",
    "\n",
    "# Using loc retrieve all the rows for the column 'DATE' and display the first 5 row\n",
    "\n",
    "# Using loc retrieve the first 5 rows and the columns NB TOTAL to TOTAL VOLUME (including columns in between)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we doing with these different patterns?  The first example is returning a dataframe that only contains\n",
    "the rows and columns that we had specified, but notice that it keeps the labels for both the rows and the columns.  \n",
    "\n",
    "The second version is actually returning a new type that we haven't talked about yet, namely a `Series` object.  We will\n",
    "discuss a series here momentarily.   \n",
    "\n",
    "The final example is also returning a dataframe, however it is using the slice operation on strings.  This is unique\n",
    "to pandas, but it understands how to work with a slice operation using the column labels, even if they are not numeric.  \n",
    "\n",
    "**NOTE:** It is important to call out that slices in pandas do not work the same as python, where python slices are none\n",
    "inclusive on the upper bound, pandas slices are inclusive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "So in our last exercise we came across a new type from pandas, the `Series`.  A series in pandas is an objec that represents\n",
    "a one-dimensional structure, much like a list.  Anytime you work with a 1D object it will be in the form of a series, \n",
    "while anytime you work with something that is 2D it will be a dataframe.  \n",
    "\n",
    "In essence, you can consider that a dataframe is a collection of columns, where each column is a series.  _This is actually\n",
    "how it is stored internally_.  \n",
    "\n",
    "When requesting the data, if you select a single row or select a single column you will end up with a series.  It is very important\n",
    "to note that series will maintain their labels whenever possible.  \n",
    "\n",
    "Once you have access to a series, like a dataframe you can use the `loc` to retrieve an exact value, but unlike a dataframe the series\n",
    "loc only accepts one dimension.  \n",
    "\n",
    "        series.loc[label]\n",
    "        \n",
    "**NOTE:** Pandas heavy utilizes the notion of method chaining.  This is done so that operations can be run without the\n",
    "need to intermediate temporary variables.  This means that you can run a `loc` from a series on the results of a `loc`\n",
    "from a dataframe.  \n",
    "\n",
    "        df.loc[:, 'NB TOTAL'].loc[3]\n",
    "\n",
    "Besides just retrieving specific value from a series, there are some other useful functions that we can work with.  \n",
    "\n",
    "- `describe()` - descriptive statiscs on data in a series\n",
    "- `max()`\n",
    "- `min()`\n",
    "- `mean()`\n",
    "- `median()`\n",
    "- `mode()`\n",
    "- `sum()`\n",
    "- `value_counts()`\n",
    "\n",
    "These methods are useful in giving us some basic insight into the data that we have available to us.  Let's try some of these\n",
    "methods out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "\n",
    "# 1. First lets describe the data for the 'SB TOTAL' column\n",
    "\n",
    "# 2. Lets use describe on the data for the `NB TOTAL` column\n",
    "\n",
    "# 3. Get the max for the 'EB TOTAL' column and assign it to eb_total_max\n",
    "\n",
    "#assert eb_total_max == 30033.0\n",
    "\n",
    "# 4. Display the top 10 unique values for the `DATE` column\n",
    "\n",
    "# 5. Get the median for the column `NB TOTAL`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, the 5th part of the above exercise should have given you an error.  Namely an error about converting a string\n",
    "to a number (the string being `no count`).  It seems like we have some bad data in our `NB TOTAL` column.  At this point, using only the\n",
    "tools above, we would need to scan through our table to find the bad columns.  \n",
    "\n",
    "Luckily, pandas dataframe offer more than just an easy way to view our data.  This power is part of the querying capabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "We haven't yet talked much about it, but pandas provides the ability to retrieve rows and columns from a sequence of booleans.  \n",
    "\n",
    "        df.loc[[True, False, False, True, False, True], 'NB TOTAL':'TOTAL VOLUME']\n",
    "        \n",
    "This allows us to create conditions for when a row or column should be returned.  What's even more powerful is the fact that\n",
    "the series object in pandas has over loaded some operators to allow a series to generate a boolean series given specific\n",
    "conditions.  For example the below command will return a boolean series where the `TOTAL VOLUME` is more than 1000.  \n",
    "\n",
    "        df.loc[:, 'TOTAL VOLUME'] > 1000\n",
    "        \n",
    "Besides the common overload operators, pandas provides methods that we can use to create boolean series based on a\n",
    "given criteria, for example the below command will return all the rows where the value for the `SB TOTAL` column is not `NaN`.  \n",
    "\n",
    "        pd.notna(df.loc[:, 'SB TOTAL'])\n",
    "        \n",
    "Other useful commands that pandas provides for querying information include.  \n",
    "\n",
    "- `pandas.isna()`\n",
    "- `pandas.isnull()`\n",
    "- `pandas.notnull()`\n",
    "\n",
    "Finally, you can combine multiple conditional queries using the `&` operator (or uses the `|` operator).  So a command\n",
    "that will retrieve rows where the `SB TOTAL` is greater than 10000 but the `TOTAL VOLUME` is less than 20000 would look like\n",
    "this.  \n",
    "\n",
    "        df.loc[(df.loc[:, 'SB TOTAL'] > 10000) & (df.loc[:, 'TOTAL VOLUME'] < 20000)].head(5)\n",
    "\n",
    "Let's start with a couple of exercises to test our knowledge of using these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5\n",
    "\n",
    "# 1. Display the first 5 rows where the TOTAL VOLUME is greater than 10000\n",
    "\n",
    "# 2. Display the first 5 rows where the NB TOTAL is not null\n",
    "\n",
    "# 3. Display the rows where there is a value in both `SB TOTAL` and in `EB TOTAL`\n",
    "\n",
    "# 4. Display the sum of 'EB TOTAL' for the rows where the `EB TOTAL` is an even number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point we have only referenced columns using the `loc` method.  Pandas does provide a few other\n",
    "formats that you can use.  Each of these below commands will all return the same value.  \n",
    "\n",
    "    df.low[:, 'DATE']\n",
    "    df['DATE']\n",
    "    df.DATE\n",
    "    \n",
    "There are also other commands that you can use besides `loc`.  \n",
    "\n",
    "    df.loc[0, 'NB TOTAL']\n",
    "    df.iloc[0, 1]\n",
    "    df.at[0, 'NB TOTAL']\n",
    "    df.iat[0, 1]\n",
    "    \n",
    "This means that the above commands could have been re-written, for example the answer for number 4 could have been written\n",
    "as:\n",
    "\n",
    "    df[df['EB TOTAL'] % 2 == 0]['EB TOTAL']\n",
    "    # or\n",
    "    df.loc[df['EB TOTAL'] % 2 == 0, 'EB TOTAL']\n",
    "    \n",
    "**NOTE:** The difference between `at` and `loc`, is that while `loc` can return a series or dataframe, `at` will only\n",
    "return a scalar, this means it does not support slicing.  \n",
    "\n",
    "**2nd NOTE:** While the different column and loc methods will return the same results, they are not the same operation\n",
    "and this has to do with pandas returning a `view` or a `copy` of the data.  But for simple quering purposes they will work the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes also have some other useful methods for querying data values or returnings a different dataframe\n",
    "to work with.  These include the following commands.  \n",
    "\n",
    "- nsmallest(n, columns) - `df.nsmallest(5, columns='TOTAL VOLUME')`\n",
    "- nlargest(n, columns) - `df.nlargest(5, columns=['TOTAL VOLUME', 'EB TOTAL'])`\n",
    "- sample(n) - A random sample of `n` instances (rows) from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "\n",
    "# 1. What is the sum of the `TOTAL VOLUME` for the 2 largest in `EB TOTAL`.  \n",
    "#   Should come up with 117,269\n",
    "\n",
    "# 2. What is the sum of the `TOTAL VOLUME` for the 3 smallest `SB TOTAL` values\n",
    "#   Should come up with 132\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "Some of you may have noticied or wondered (or possibly even asked a question) about why we aren't using `NB TOTAL`.  If\n",
    "you recall from the very beginning, this column was of type `object` and not a numeric type, which means we can't use\n",
    "or aggregate functions against that column until we fix the data.  \n",
    "\n",
    "For this next section we are going to go through our data in an attempt to either remove or adjust results where\n",
    "appropriate.  \n",
    "\n",
    "Now there are a lot of things that we can do to our data to clean it up.  The first thing that I want to do is adjust\n",
    "the names of the colums that we are using so that I can use `nb` instead of `NB TOTAL` for all of my commands.  Pandas\n",
    "provides a few ways for us to do this, either by assigning the new names to the `columns` property of the dataframe\n",
    "or by using the `rename` method.  \n",
    "\n",
    "In this next exercise, rename the columns in our dataframe according to the map below.  \n",
    "\n",
    "- `24 HOUR VOLUME COUNT LOCATIONS` -> `location`\n",
    "- `NB TOTAL` -> `nb`\n",
    "- `SB TOTAL` -> `sb`\n",
    "- `EB TOTAL` -> `eb\n",
    "- ` WB TOTOAL` -> `wb`\n",
    "- `TOTAL VOLUME` -> `total`\n",
    "- `DATE` -> `date`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7\n",
    "\n",
    "# 1. Rename all the columns according to the list above\n",
    "\n",
    "# 2. Select the first 5 instances where the `sb` value is less than 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our next cleanup task is going to be a little more involved, first we need to find all the \n",
    "data in the `nb` column that is a non-numeric, the columns that are stopping us from using our\n",
    "aggregate functions.  \n",
    "\n",
    "If we were using standard python I could compare the value to see if it can be cast to number\n",
    "or not.  Since this is a common operation, pandas provides this ability for us, but using\n",
    "the `str` property of a series, it will give us the functions provided by pandas for string\n",
    "types, one of which is the `isnumeric()` function.  There are a few others, some of which\n",
    "are listed here.  \n",
    "\n",
    "- `str.contains()`\n",
    "- `str.endswith()`\n",
    "- `str.len()`\n",
    "- `str.isdigit()`\n",
    "- `str.isalpha()`\n",
    "- `str.isspace()`\n",
    "\n",
    "Using the above commands is done as demonstrated below.  \n",
    "\n",
    "    df['location'].str.contains('Barton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8\n",
    "\n",
    "# 1. Create a variable `na_rows` that contains a boolean series where nb is NaN\n",
    "na_rows = pd.isna(df['nb'])\n",
    "\n",
    "# 1. Create a variable `good_rows` that contains a boolean series where nb is NaN or numeric\n",
    "number_rows =  (na_rows | df['nb'].str.isnumeric())\n",
    "\n",
    "# 3. Display the rows where nb is not numeric or NaN (note you can use ~ to negate a boolean series)\n",
    "df[~number_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have our bad rows, and it looks like we have the following errors.  \n",
    "\n",
    "- 2 rows `during rd. construction`\n",
    "- 3 rows `no count`\n",
    "\n",
    "At this point we have a decision to make.  This data will obviously not work as we need our `nb` column\n",
    "to contain numeric data, so we can do one of the following.  \n",
    "\n",
    "1. Set the bad values to `NaN`\n",
    "2. Set the bad values to 0\n",
    "3. Set the bad values to the mean of the column\n",
    "4. Drop the rows with the bad values\n",
    "\n",
    "Which option we choose will larger depend upon whether we need the other information provided and if we\n",
    "feel that using any of those options could drastically skew our perception of the data.  For this exercise\n",
    "we are going to simply drop these rows.  This can be done by using the `drop` function.  \n",
    "\n",
    "    df.drop(index=[row1, row2, row10])\n",
    "    \n",
    "Lets go ahead and run the drop command and then describe the dataframe and output the last 5 results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9\n",
    "\n",
    "# 1. Get the indexes of the rows to drop using the number_rows column above (remember to negate it) store in variable drop_rows\n",
    "\n",
    "# 1. Run the drop command on the rows from the inverted boolean variable number_rows, save the results in dropped_df\n",
    "\n",
    "# 2. Display the describe output for the dropped_df dataframe for column nb\n",
    "\n",
    "# 3. Display the rows 470 to 478\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our last cleanup step, lets now fix a couple of our types.  Right now we have dropped the columns from `nb`\n",
    "that were causing it to not automatically be discovered as a number so we can now convert that column.  But\n",
    "the other column that is also an incorrect type is the `date` column which is currently of type `object` but\n",
    "can be of type `datetime`.  \n",
    "\n",
    "To convert an entire series, you can use the provided pandas methods:\n",
    "\n",
    "- `pandas.to_numeric(series)`\n",
    "- `pandas.to_datetime(series)`\n",
    "\n",
    "If we want to update a column, we can do so by selecting the column using the `loc` on a dataframe.  After selecting\n",
    "you need to supply a scalar (single value) or a series or sequence with the same number of instances (rows) as the\n",
    "dataframe.  This is done simply by specifying the column to overwrite and the values to apply.  \n",
    "\n",
    "    df['nb'] = new_values\n",
    "\n",
    "For this next exercise lets create a converted series that is of the correct types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10\n",
    "\n",
    "# 1. Create a new series that is the nb series converted to numeric, store in number_nb\n",
    "\n",
    "# 2. Create a new series that is the date series converted to a datetime, store in datetime_date\n",
    "\n",
    "# 3. Assign the updated series to both the nb and date columns\n",
    "\n",
    "# 4. Display the dtypes of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have now cleaned up our datatypes, but we still have one thing that could cause us some problems, namely we\n",
    "have `NaN` in a lot of places that if we try to run our own calculations against are going to cause issues.  So what\n",
    "we want to do is to convert all the `NaN` values to 0.  In doing this we are, however, possibly lossing some information,\n",
    "namely which direction a road is traveling.  \n",
    "\n",
    "Looking over the data it seems that a road is considered traveling north to south when there is a value in either the\n",
    "`nb` or the `sb` columns.  The same thing goes for `eb` and `wb`.  So what we want to do is create two more columns\n",
    "on our dataframe.  These columns are going to be boolean columns and contain True when the direction traveled is either\n",
    "`nb` or `sb` and `eb` or `wb`.  \n",
    "\n",
    "Finally pandas provides a helper function that will automatically fill in all entries in a dataframe with a supplied value\n",
    "when the original value in the cell is `NaN`, this command is `fillna()` on the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 11\n",
    "\n",
    "# 1. Create a boolean series named ns_traffic that is true when either nb or sb is not NaN\n",
    "\n",
    "# 2. Create a boolean series named ew_traffic that is true when either wb or eb is not NaN\n",
    "\n",
    "# 3. Create a new column on the dataframe named `ns` and assign the ns_traffic series\n",
    "\n",
    "# 4. Create a new column on the dataframe named `ew` and assign the ew_traffic series\n",
    "\n",
    "# 5. Using the fillna function to set all the NaN values to 0 and save the results in zero_df\n",
    "\n",
    "# 6. Display the first 5 rows of the zero_df dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "At this point we now have a dataframe that has been cleaned and would be ready for further inspection\n",
    "or even possible use in a machine learning model.  (__Further vectorization likely\n",
    "required__).  \n",
    "\n",
    "To make sure that we persist our changes we can store our results back to the filesystem in csv\n",
    "format to be loaded later.  This is done in the following cell.  \n",
    "\n",
    "For our next notebook lets look at the visual capabilities that are provided by pandas, especially\n",
    "when used with timeseries data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_name = os.path.join('..', '.data', 'austin_cleaned.csv')\n",
    "zero_df.to_csv(save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
