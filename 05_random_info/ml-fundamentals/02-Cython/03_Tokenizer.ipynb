{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Tokenizing is a very difficult task.\n",
    "You need to take a single string and break it into individual tokens.\n",
    "\n",
    "We could do it in pure Python, but let's try using Cython to get performance gains.\n",
    "\n",
    "First let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../.data\n",
    "cd ../.data\n",
    "# https://s3-us-west-2.amazonaws.com/resero2/datasets/ml-foundations/emoji_tweets_5k.csv\n",
    "if [ ! -f emoji_tweets_5k.csv ]; then\n",
    "    echo \"File not found. Downloading from s3\"\n",
    "    wget -q https://s3-us-west-2.amazonaws.com/resero2/datasets/ml-foundations/emoji_tweets_5k.csv\n",
    "else\n",
    "    echo \"File exists, not downloading form s3\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open up the data file and turn it into two individual lists.\n",
    "\n",
    "We will have our tweets and emoji targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "texts = []\n",
    "emojis = []\n",
    "\n",
    "with open(\"../.data/emoji_tweets_5k.csv\") as infile:\n",
    "    for row in csv.reader(infile):\n",
    "        text = json.loads(row[1]).strip()\n",
    "        texts.append(text)\n",
    "        emojis.append(json.loads(row[2]))\n",
    "\n",
    "print(f'Text count: {len(texts)}')\n",
    "print(f'Emojis count: {len(emojis)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an analyzer.\n",
    "\n",
    "The analyzer will take in a tokenizer and then be able to tokenize a list of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "class Analyzer:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def analyze_all(self, tweets: Iterable[str])-> Iterable[Iterable[str]]:\n",
    "        return [self.analyze(tweet) for tweet in tweets]\n",
    "            \n",
    "    def analyze(self, tweet_text: str) -> Iterable[str]:\n",
    "        tokens = self.tokenizer(tweet_text)\n",
    "        return list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets make a simple regex tokenizer that splits on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def simple_re_tokenizer(text):\n",
    "    return re.compile('\\s').split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(simple_re_tokenizer)\n",
    "%timeit analyzer.analyze_all(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex in Cython\n",
    "~10 ms isn't bad. But let's see if we can improve that using the C `regex.h` library.\n",
    "\n",
    "This was pretty gnarly code so I won't make you implement it.\n",
    "Let's just walk through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "# import regex.h and it's functions/structs/dtypes\n",
    "# See more on importing C & C++ headers here https://cython.readthedocs.io/en/latest/src/userguide/external_C_code.html#referencing-c-header-files\n",
    "cdef extern from \"regex.h\" nogil:\n",
    "    ctypedef struct regmatch_t:\n",
    "       int rm_so\n",
    "       int rm_eo\n",
    "    ctypedef struct regex_t:\n",
    "       pass\n",
    "    int REG_EXTENDED\n",
    "    int regcomp(regex_t* preg, const char* regex, int cflags)\n",
    "    int regexec(const regex_t *preg, const char *string, size_t nmatch, regmatch_t pmatch[], int eflags)\n",
    "    void regfree(regex_t* preg) \n",
    "\n",
    "cdef regex_split(bytes pageContent, bytes regex):\n",
    "    cdef int end_position\n",
    "    cdef list results = list()\n",
    "    cdef regex_t regex_obj\n",
    "    cdef regmatch_t regmatch_obj[1]\n",
    "    cdef int regex_res = 0\n",
    "    cdef int current_str_pos = 0\n",
    "    \n",
    "    regcomp(&regex_obj, regex, REG_EXTENDED)\n",
    "    regex_res = regexec(&regex_obj, pageContent[current_str_pos:], 1, regmatch_obj, 0)\n",
    "\n",
    "    while regex_res == 0:\n",
    "        if regmatch_obj[0].rm_so > 1:\n",
    "            end_position = current_str_pos + regmatch_obj[0].rm_so\n",
    "            results.append(pageContent[current_str_pos : end_position])\n",
    "\n",
    "        current_str_pos += regmatch_obj[0].rm_eo\n",
    "        regex_res = regexec(&regex_obj, pageContent[current_str_pos:], 1, regmatch_obj, 0)\n",
    "    cdef int bytes_len = len(pageContent)\n",
    "    if current_str_pos != bytes_len:\n",
    "        results.append(pageContent[current_str_pos : bytes_len])\n",
    "    regfree(&regex_obj)\n",
    "    return results\n",
    "\n",
    "def cython_whitespace_tokenize(text):\n",
    "    return [t.decode('utf8') for t in regex_split(text.encode('utf8'), b'\\s')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elvish](https://ci.memecdn.com/5509375.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cython_whitespace_tokenize(\"Hello how are you\\n I'm fine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the performance of our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(cython_whitespace_tokenize)\n",
    "%timeit analyzer.analyze_all(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that our custom `C` regex was not as fast as Pythons regex.\n",
    "\n",
    "Pythons regex runs very fast.\n",
    "Not fast considering it's python, but fast compared to other languages.\n",
    "You can also import re2 which runs 60% faster than the standard library re module.\n",
    "\n",
    "Let's try building our own splitter in cython that splits on the space character `' '`.\n",
    "To make things easier, you can access the c++ stdlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_whitespace_tokenizer(text):\n",
    "    '''\n",
    "    split on whitespace\n",
    "    '''\n",
    "#   We could just use return text.split(' ') but we want code that is closer to cython to make it easier to write Cython code.\n",
    "    results = []\n",
    "    last_whitespace = 0\n",
    "    length = len(text)\n",
    "    for i in range(length):\n",
    "        if text[i] == ' ':\n",
    "            if last_whitespace < i:\n",
    "                results.append(text[last_whitespace : i])\n",
    "            last_whitespace = i + 1\n",
    "    if last_whitespace < length:\n",
    "        results.append(text[last_whitespace : i+1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_whitespace_tokenizer(text):\n",
    "    '''\n",
    "    split on whitespace\n",
    "    '''\n",
    "    # We could just use return text.split(' ') but we want code that is closer to cython to make it easier to write Cython code.\n",
    "    # So try to use for loops for this example\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(simple_whitespace_tokenizer)\n",
    "%timeit analyzer.analyze_all(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the same code in Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cdef c_tokenize(char *text):\n",
    "    cdef int last_whitespace = 0\n",
    "    cdef int length = len(text)\n",
    "    cdef int i\n",
    "    results = []\n",
    "    for i in range(length):\n",
    "        if text[i] == b' ':\n",
    "            if last_whitespace < i:\n",
    "                results.append(text[last_whitespace : i])\n",
    "            last_whitespace = i + 1\n",
    "    if last_whitespace < length:\n",
    "        results.append(text[last_whitespace : i+1])\n",
    "    return results\n",
    "\n",
    "def cython_space_tokenize(text):\n",
    "    return [t.decode('utf8') for t in c_tokenize(text.encode('utf8'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# distutils: language = c++\n",
    "\n",
    "# Import c++ classes from the standard library, if you need them\n",
    "from libcpp.pair cimport pair\n",
    "from libcpp.string cimport string\n",
    "from libcpp.vector cimport vector\n",
    "\n",
    "cdef c_tokenize(char *text):\n",
    "    # split characters where the char == b' '\n",
    "    # return a python object of byte arrays, i.e. [b'hi', b'I'm', b'bob']\n",
    "    # remember that you can implement this in pure python, then slowly change variables to c variables for speed\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def cython_space_tokenize(text):\n",
    "#     take the python string and convert it to utf8 encoded bytes\n",
    "    return [t.decode('utf8') for t in c_tokenize(text.encode('utf8'))]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = Analyzer(cython_space_tokenize)\n",
    "%timeit analyzer.analyze_all(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we are now just half the speed of Pythons regex.\n",
    "But that isn't a fair comparison. So let's see how fast python is if we split on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_space_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "analyzer = Analyzer(simple_space_tokenizer)\n",
    "%timeit analyzer.analyze_all(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Sometimes Python is just fast. String manipulation in Python is very fast.\n",
    "It is often better to look at performance and then decide if you need to optimize.\n",
    "\n",
    "Don't prematurely optimize.\n",
    "\n",
    "Use a profiler to look for hotspots in code if you need performance gains.\n",
    "The builtin cprofiler is a great tool to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
