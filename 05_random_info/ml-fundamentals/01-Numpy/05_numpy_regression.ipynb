{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. NumPy Project: Predicting Emojis with Regression\n",
    "\n",
    "August 26, 2018\n",
    "\n",
    "We've now learned enough to be dangerous! \n",
    "\n",
    "Let's use this knowledge to implement two famous algorithms:\n",
    "\n",
    "* Ranking Web Pages with PageRank\n",
    "* **Predicting Emojis in Tweets with Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f'numpy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis in tweets\n",
    "\n",
    "<img src=\"http://incrediblethings.com/wp-content/uploads/2014/12/top-100-twitter-emojis-cut-e1418995524941.jpg\" width=\"400\"/>\n",
    "\n",
    "The problem statement is as follows: Given a tweet with its emojis removed, can we predict which emojis were present?\n",
    "\n",
    "If we can produce an effective solution to this problem, then we have created a natural language processor which can infer sentiment, or emotion, from text.\n",
    "\n",
    "### Example\n",
    "\n",
    "Original Tweet:\n",
    "\n",
    "> @gautam_rode4 @TatitaSim Amen üôèüôèüôè thank you bro\n",
    "\n",
    "Modified Tweet:\n",
    "> @gautam_rode4 @TatitaSim Amen thank you bro\n",
    "\n",
    "Prediction Target:\n",
    "> üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Some Data\n",
    "\n",
    "For this project we have collected around 5 million tweets with emojis. For this lesson, however, we'll work with a small subset of this data that has only 5,000.\n",
    "\n",
    "Run this cell to download the CSV file into the current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "\n",
    "url = \"https://s3-us-west-2.amazonaws.com/resero2/datasets/ml-foundations/emoji_tweets_5k.csv\"\n",
    "\n",
    "print('Downloading data...')\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "with open(\"emoji_tweets_5k.csv\", 'wb') as outfile:\n",
    "    shutil.copyfileobj(response.raw, outfile)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data using the csv library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "texts = []\n",
    "emojis = []\n",
    "\n",
    "with open(\"emoji_tweets_5k.csv\") as infile:\n",
    "    for row in csv.reader(infile):\n",
    "        text = json.loads(row[1]).strip()\n",
    "        texts.append(text)\n",
    "        emojis.append(json.loads(row[2]))\n",
    "\n",
    "print(f'Text count: {len(texts)}')\n",
    "print(f'Emojis count: {len(emojis)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i, emojis[i], texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "Because we don't have a very large dataset, let's not attempt to predict all emojis that we find but instead only the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "emoji_counter = Counter()\n",
    "for emoji_dict in emojis:\n",
    "    emoji_counter.update(emoji_dict.keys())\n",
    "\n",
    "print(emoji_counter.most_common(10))\n",
    "\n",
    "common_emojis = [e[0] for e in emoji_counter.most_common(10)]\n",
    "\n",
    "print()\n",
    "print('Top 10:')\n",
    "print(common_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least-Squares Regression\n",
    "\n",
    "You may be wondering how we're going to use linear regression to predict emojis, but just bear with us a bit ;-)\n",
    "\n",
    "### Univariate Least Squares\n",
    "\n",
    "In its simplest form with one independent variable, $x$, and one target, $y$, linear least-squares regression finds a \"best fit\" prediction line that minimizes the sum of squared residuals between the prediction and the data:\n",
    "\n",
    "$$ \\underset{w, b}{\\mathrm{argmin}} \\sum_{i}{\\left[y_{i} - f(x_{i}, w, b)\\right]^2} $$\n",
    "\n",
    "$$ f(x, w, b) = b + wx $$\n",
    "\n",
    "where $w$ is the weight (or slope) and $b$ is the the bias (or offset, or intercept).\n",
    "\n",
    "We can show this visually with some artificial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = np.random.rand(500, 1)\n",
    "noiseless_y = 3 * X + 2\n",
    "y = noiseless_y + np.random.randn(500, 1)\n",
    "plt.scatter(X, y)\n",
    "f = plt.plot(X, noiseless_y, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Least Squares\n",
    "\n",
    "Univariate regression can be extended to incorporate multiple independent variables by simply adding a weight for each input.\n",
    "\n",
    "We will represent the input vector, $\\mathbf{x}$, and also the weight vector, $\\mathbf{w}$, in bold font to distinguish them from scalars:\n",
    "\n",
    "$$ \\underset{\\mathbf{w}, b}{\\mathrm{argmin}} \\sum_{i}{[y_{i} - f(\\mathbf{x}_{i}, \\mathbf{w}, b)]^2} $$\n",
    "\n",
    "$$ f(\\mathbf{x}, \\mathbf{w}, b) = b + \\mathbf{w}^T\\mathbf{x} $$\n",
    "\n",
    "Here $\\mathbf{w}^T\\mathbf{x}$ is the dot product, which sums the product of each element in $\\mathbf{x}$ with the corresponding weight in $\\mathbf{w}$.\n",
    "\n",
    "$$ \\begin{bmatrix} w_{1} & w_{2} & ... & w_{n} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ ... \\\\ x_{n} \\end{bmatrix} \\quad = \\quad \\sum_{i} w_{i} x_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Solution\n",
    "\n",
    "There are several ways one can find the best parameters $\\mathbf{w}$ and $b$ to minimize the objective function above. A common technique is to solve the \"normal equations\".\n",
    "\n",
    "In the equations below, $m$ is the number of data points (instances) and $n$ is the number of independent variables (features).\n",
    "\n",
    "First we will combine the weights and the bias into the single vector, $\\mathbf{\\beta}$:\n",
    "\n",
    "$$ \\mathbf{\\beta} = \\begin{bmatrix} b \\\\ w_{1} \\\\ w_{2} \\\\... \\\\ w_{n} \\end{bmatrix} $$\n",
    "\n",
    "Next we will place each input vector into the row of a matrix, $\\mathbf{X}$, and prepend a 1 for each instance which will be multiplied by the bias in the $\\mathbf{\\beta}$ vector.\n",
    "\n",
    "$$ \\mathbf{X} = \\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & ... & X_{1,n} \\\\ 1 & X_{2,1} & X_{2,2} & ... & X_{2,n} \\\\ ... & ... & ... & ... \\\\ 1 & X_{m,1} & X_{m,2} & ... & X_{m,n} \\end{bmatrix} $$\n",
    "\n",
    "You can see now that the bias term in $\\beta$ and the constant column of all 1s will be mutliplied together to accomplish adding in the bias term in a single matrix multiply:\n",
    "\n",
    "$$ \\mathbf{X} \\mathbf{\\beta} = \\quad \\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & ... & X_{1,n} \\\\ 1 & X_{2,1} & X_{2,2} & ... & X_{2,n} \\\\ ... & ... & ... & ... \\\\ 1 & X_{m,1} & X_{m,2} & ... & X_{m,n} \\end{bmatrix} \\begin{bmatrix} b \\\\ w_{1} \\\\ w_{2} \\\\... \\\\ w_{n} \\end{bmatrix} $$\n",
    "\n",
    "Finally we place our targets in a column vector $\\mathbf{y}$.\n",
    "\n",
    "$$ \\mathbf{y} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ ... \\\\ y_{m} \\end{bmatrix} $$\n",
    "\n",
    "In this construction, we can now write our objective as:\n",
    "\n",
    "$$ \\underset{\\mathbf{w}, b}{\\mathrm{argmin}} \\quad \\| \\mathbf{y} -\\mathbf{X} \\mathbf{\\beta} \\|^2 $$\n",
    "\n",
    "where $\\| \\|^2$ is the squared norm, or sum of squares.\n",
    "\n",
    "By taking the derivative of this expression and setting it equal to zero (calculus not shown for brevity), we end up with:\n",
    "\n",
    "$$ \\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X} \\mathbf{\\beta} $$\n",
    "\n",
    "and so:\n",
    "\n",
    "$$ \\mathbf{\\beta} = ( \\mathbf{X}^T \\mathbf{X} )^{-1} \\mathbf{X}^T \\mathbf{y} $$\n",
    "\n",
    "where the superscript $^{T}$ denotes the transpose and superscript $^{-1}$ denotes the matrix inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement It!\n",
    "\n",
    "Using the formulation above, implement the function below which finds the best fit parameters $\\mathbf{\\beta}$. Then use the function to find the best fit to the artificial data we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sample Implementation\n",
    "\n",
    "def least_squares(X, y):\n",
    "    \"\"\"\n",
    "    :param X: a 2d ndarray with shape (m, n) holding the independent variables\n",
    "    :param y: a 2d ndarray with shape (m, 1) holding the targets\n",
    "    :returns: a 2d ndarray with shape (n+1, 1) holding the bias (first element) and the weights (rest of the elements)\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert y.shape[0] == m\n",
    "    \n",
    "    # Augment X\n",
    "    ones_col = np.ones([m, 1])\n",
    "    X = np.hstack([ones_col, X])\n",
    "\n",
    "    # Solve the normal equations\n",
    "    result_one = np.linalg.inv(np.matmul(X.T, X))\n",
    "    result_two = np.matmul(result_one, X.T)\n",
    "    return np.matmul(result_two, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X, y):\n",
    "    \"\"\"\n",
    "    :param X: a 2d ndarray with shape (m, n) holding the independent variables\n",
    "    :param y: a 2d ndarray with shape (m, 1) holding the targets\n",
    "    :returns: a 2d ndarray with shape (n+1, 1) holding the bias (first element) and the weights (rest of the elements)\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert y.shape[0] == m\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it on our synthetic data\n",
    "X = np.random.rand(500, 1)\n",
    "noiseless_y = 3 * X + 2\n",
    "y = noiseless_y + np.random.randn(500, 1)\n",
    "\n",
    "beta = least_squares(X, y)\n",
    "bias = beta[0]\n",
    "weights = beta[1:]\n",
    "print(f'bias: {bias}, weights: {weights}')\n",
    "\n",
    "plt.scatter(X, y)\n",
    "predictions = weights[0] * X + bias\n",
    "f = plt.plot(X, predictions, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that we implemented the solution by using a matrix inversion, which in practice isn't the best method in terms of numerical stability. A better approach would instead use **np.linalg.solve()**, but we will leave that as an exercise for the reader!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification via Regression\n",
    "\n",
    "<img src=\"http://mlpy.sourceforge.net/docs/3.2/_images/lda_binary1.png\" />\n",
    "\n",
    "So how does linear regression relate to machine learning? Well, one way to formulate a binary classification problem is to encode one class as y=0 and the other class as y=1.\n",
    "\n",
    "The plotting code below generates two synthetic classes with 2 independent variables (features). We create an $\\mathbf{X}$ matrix, a $\\mathbf{y}$ vector with our encoded class targets, and then run the linear regression calculation.\n",
    "\n",
    "$$ \\mathbf{X} = \\begin{bmatrix} 0.4 & 0.2 \\\\ 0.3 & 0.1 \\\\ ... & ...\\\\ 0.5 & 0.8 \\\\ 0.6 & 0.7 \\end{bmatrix}\n",
    "\\quad \\mathbf{y} = \\begin{bmatrix} 0 \\\\ 0 \\\\ ... \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We color the points based on the model prediction, and also fill in the volume with semi-transparent cubes colored this way so that you can see the separating hyperplane. You could imagine creating a threshold on the score (corresponding to roughly halfway between red and blue) in order to get a discrete 0/1 class prediction.\n",
    "\n",
    "Be sure to click and drag the 3D plot to rotate it around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Generate two classes with 2d gaussian distributions\n",
    "x1 = np.random.randn(50, 2) * 0.1 + (0.25, 0.25)\n",
    "x2 = np.random.randn(50, 2) * 0.1 + (0.75, 0.75)\n",
    "X = np.vstack([x1, x2])\n",
    "\n",
    "# Generate class indicators\n",
    "y = np.array([0]*50 + [1]*50)\n",
    "\n",
    "# Fit least squares model\n",
    "beta = least_squares(X, y)\n",
    "bias = beta[0]\n",
    "weights = beta[1:]\n",
    "predictions = np.matmul(X, weights) + bias\n",
    "\n",
    "# Render least squares model\n",
    "markers = dict(\n",
    "    size=5,\n",
    "    color=predictions,\n",
    "    colorscale='Jet',   # choose a colorscale\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "trace = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=y, mode='markers', marker=markers)\n",
    "\n",
    "# Fill volume with semi-transparent cubes\n",
    "lin = np.linspace(0, 1, 11)\n",
    "x, y, z = np.meshgrid(lin, lin, lin)\n",
    "x = x.ravel(); y = y.ravel(); z = z.ravel(); \n",
    "fill_predictions = np.matmul(np.vstack([x, y]).T, weights) + bias\n",
    "\n",
    "fill_markers = dict(\n",
    "    size=20,\n",
    "    color=fill_predictions,\n",
    "    colorscale='Jet',   # choose a colorscale\n",
    "    opacity=0.03,\n",
    "    symbol='square'\n",
    ")\n",
    "\n",
    "fill_trace = go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=fill_markers)\n",
    "\n",
    "# Plot\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace, fill_trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Emojis\n",
    "\n",
    "### Creating the X Matrix\n",
    "In order to represent our twitter data in a form convenient for regression, we're going to transform each tweet text in a \"binary bag of words\" representation.\n",
    "\n",
    "A \"binary bag of words\" simply means that we will create an independent variable (a feature) for each word in a vocabulary, and set that feature to 1 if the tweet contains the word. \n",
    "\n",
    "An example encoding for two sentences and small 3-word vocabulary is:\n",
    "\n",
    "* sentence one: \"the quick fox\"\n",
    "* sentence two: \"the fox\"\n",
    "\n",
    "$$ \\begin{matrix} the & quick & fox \\end{matrix} \\\\\n",
    "\\begin{bmatrix} 1 \\quad & 1 \\quad & 1 \\\\ 1 \\quad & 0 \\quad & 1 \\end{bmatrix} $$\n",
    "\n",
    "The code for creating a vocabulary is below, use it to create an $\\mathbf{X}$ matrix suitable for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # very basic regex tokenization\n",
    "    text = text.replace(\"‚Äô\", \"'\")\n",
    "    tokens = [t.lower() for t in re.split(r\"[^a-zA-Z0-9'&]+\", text)]\n",
    "    tokens = [t for t in tokens if len(t) > 0 and t != 't' and t != 'https' and t != 'co']\n",
    "    return tokens\n",
    "\n",
    "def create_vocab(texts, size):\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        words = tokenize(text)\n",
    "        word_counts.update(words)\n",
    "    return [e[0] for e in word_counts.most_common(size)]\n",
    "\n",
    "vocab = create_vocab(texts, 300)\n",
    "\n",
    "# Two useful lookup tables for converting words into integer indexes and vice-versa\n",
    "word_to_index = {w : i for i, w in enumerate(vocab)}\n",
    "index_to_word = {i : w for i, w in enumerate(vocab)}\n",
    "\n",
    "print(f'Vocab: {vocab}')\n",
    "print()\n",
    "print(f'Top 10 word to index entries: {[e for e in list(word_to_index.items())[:10]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code below to implement the X matrix creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sample Implementation\n",
    "\n",
    "def create_X_matrix(texts):\n",
    "    \n",
    "    # Initialize X to all zeros\n",
    "    m = len(texts)\n",
    "    n = len(vocab)\n",
    "    X = np.zeros([m, n])\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        for word in tokenize(text):\n",
    "            index = word_to_index.get(word.lower(), None)\n",
    "            if index is not None:\n",
    "                X[i, index] = 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "X = create_X_matrix(texts)\n",
    "#print(f'First row: {X[0]}')\n",
    "print(f'Total number of ones in X: {np.sum(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_matrix(texts):\n",
    "    \n",
    "    # Initialize X to all zeros\n",
    "    m = len(texts)\n",
    "    n = len(vocab)\n",
    "    \n",
    "    X = # **** Your code here ****\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        for word in tokenize(text):\n",
    "            index = word_to_index.get(word.lower(), None)\n",
    "            if index is not None:\n",
    "                # **** Your code here ****\n",
    "    \n",
    "    return X\n",
    "\n",
    "X = create_X_matrix(texts)\n",
    "print(f'First row: {X[0]}')\n",
    "print(f'Total number of ones in X: {np.sum(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Targets\n",
    "\n",
    "We have our $\\mathbf{X}$ matrix now, but what about the target vector $\\mathbf{y}$? There are multiple emojis, and each tweet can have more than one type of emoji. How do we handle multiple targets? Our least squares example only showed how to predict a single target.\n",
    "\n",
    "The solution is quite simple: instead of a $\\mathbf{y}$ vector with a single column, we will create a $\\mathbf{Y}$ matrix with one column per emoji. We'll use a target of 1 when an emoji exists, and a target of 0 when it does not. The math works out the same and we don't even need to alter our code!\n",
    "\n",
    "An example of encoding two emoji targets is below:\n",
    "\n",
    "$$ \n",
    "\\begin{matrix} the & quick & fox \\end{matrix} \\quad \\quad \\begin{matrix} üòç & \\quad üî•\\end{matrix} \\\\\n",
    "\\begin{bmatrix} 1 \\quad & 1 \\quad & 1 \\\\ 1 \\quad & 0 \\quad & 1 \\end{bmatrix} \\quad \\quad \\begin{bmatrix} 1 \\quad & 1 \\\\ 1 \\quad & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Write code for this encoding below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sample implementation\n",
    "\n",
    "# Two useful lookup tables for converting emojis into integer indexes and vice-versa\n",
    "emoji_to_index = {w : i for i, w in enumerate(common_emojis)}\n",
    "index_to_emoji = {i : w for i, w in enumerate(common_emojis)}\n",
    "\n",
    "def create_Y_matrix(emojis):\n",
    "    n = len(common_emojis)\n",
    "    m = len(emojis)\n",
    "    Y = np.zeros([m, n])\n",
    "    for i, single_tweet_emojis in enumerate(emojis):\n",
    "        for emoji in single_tweet_emojis:\n",
    "            index = emoji_to_index.get(emoji, None)\n",
    "            if index is not None:\n",
    "                Y[i, index] = 1\n",
    "    return Y\n",
    "                \n",
    "Y = create_Y_matrix(emojis)\n",
    "\n",
    "print(f'Y shape: {Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two useful lookup tables for converting emojis into integer indexes and vice-versa\n",
    "emoji_to_index = {w : i for i, w in enumerate(common_emojis)}\n",
    "index_to_emoji = {i : w for i, w in enumerate(common_emojis)}\n",
    "\n",
    "def create_Y_matrix(emojis):\n",
    "    raise Exception('Implement me!')\n",
    "                \n",
    "Y = create_Y_matrix(emojis)\n",
    "\n",
    "print(f'Y shape: {Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our data ready to go, but how do we determine how well the model performs? A good way to do this is to withold some of the data so that the model can't train on it.\n",
    "\n",
    "We'll fit our model on the training set, and then use it to predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "num_train = int(N * 0.9)\n",
    "\n",
    "def split(array, split_point=num_train):\n",
    "    return array[:split_point], array[split_point:]\n",
    "\n",
    "X_train, X_test = split(X)\n",
    "Y_train, Y_test = split(Y)\n",
    "texts_train, texts_test = split(texts)\n",
    "emojis_train, emojis_test = split(emojis)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print()\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "print(f'Y_test shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Check\n",
    "\n",
    "To make sure everything is correct, let's look at a tweet and its row in X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Text: {texts[-1]}')\n",
    "print(f'Emojis: {emojis[-1]}')\n",
    "print(f'X row: {X[-1]}')\n",
    "print(f'Y row: {Y[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bag of words:')\n",
    "print([index_to_word[i] for i, entry in enumerate(X[-1]) if entry > 0])\n",
    "\n",
    "print()\n",
    "print('Emoji targets:')\n",
    "print([index_to_emoji[i] for i, entry in enumerate(Y[-1]) if entry > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "\n",
    "We have everything we need now. \n",
    "\n",
    "Let's run your least_squares() function to fit a model on X_train and Y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Betas = least_squares(X_train, Y_train)\n",
    "Biases = Betas[0, :]\n",
    "Weights = Betas[1:, :]\n",
    "\n",
    "print(f'Betas shape: {Betas.shape}')\n",
    "print(f'Biases shape: {Biases.shape}')\n",
    "print(f'Weights shape: {Weights.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict With the Model\n",
    "\n",
    "We now need a method that can use the model parameters to predict on new data. Implement the predict() function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sample Implementation\n",
    "def predict(X, Betas):\n",
    "    \"\"\"\n",
    "    :param X: a 2d ndarray with shape (m, n) holding the independent variables\n",
    "    :param Betas: a 2d ndarray with shape (n+1, k) holding the parameters of a linear model (the first row contains bias terms)\n",
    "    :returns: a 2d ndarray with shape (m, k) holding the predictions\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert Betas.shape[0] == n + 1\n",
    "    \n",
    "    # Augment X\n",
    "    ones_col = np.ones([m, 1])\n",
    "    X = np.hstack([ones_col, X])\n",
    "\n",
    "    # Apply model\n",
    "    return np.matmul(X, Betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Betas):\n",
    "    \"\"\"\n",
    "    :param X: a 2d ndarray with shape (m, n) holding the independent variables\n",
    "    :param Betas: a 2d ndarray with shape (n+1, k) holding the parameters of a linear model (the first row contains bias terms)\n",
    "    :returns: a 2d ndarray with shape (m, k) holding the predictions\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    assert Betas.shape[0] == n + 1\n",
    "    \n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = predict(X_test, Betas)\n",
    "print(f'Y_test_pred shape: {Y_test_pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Print results when an emoji prediction score exceeds a threshold (0.4)\n",
    "# You'll need to scroll down a ways before you see a prediction that isn't üòÇ\n",
    "#\n",
    "for test_text, test_emoji, y_pred in zip(texts_test, emojis_test, Y_test_pred):\n",
    "    highest_scoring_emoji_index = np.argmax(y_pred)\n",
    "    highest_score = y_pred[highest_scoring_emoji_index]\n",
    "    if highest_score > 0.4:\n",
    "        print('-'*40)\n",
    "        print('Text:', test_text)\n",
    "        print('Bag of words:', [w.lower() for w in test_text.split() if w.lower() in vocab])\n",
    "        print('Common emojis:', [e for e in test_emoji if e in common_emojis])\n",
    "        print(sorted(zip(y_pred, common_emojis), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words with the Highest Weights\n",
    "\n",
    "While not a great model, it does seem to have learned some important correlations between words and emojis. Let's look at the words with the highest absolute weights for the heart emoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_important_words(emoji, count=10):\n",
    "    emoji_index = emoji_to_index[emoji]\n",
    "    emoji_betas = Betas[:, emoji_index]\n",
    "    emoji_word_weights = emoji_betas[1:] # first term is bias\n",
    "    sorted_idxs = np.argsort(np.abs(emoji_word_weights))[::-1]\n",
    "    for idx in sorted_idxs[:count]:\n",
    "        print(emoji_word_weights[idx], '\\t', index_to_word[idx])\n",
    "\n",
    "print_important_words('‚ù§')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting Thoughts\n",
    "\n",
    "This linear bag of words model is interesting but doesn't seem that great. It appears to have learned a few nice word correlations , but it seems to predict üòÇtoo often.\n",
    "\n",
    "How could we make this better? Here are a few ideas:\n",
    "\n",
    "1. Use more training data, for example the full set of 5M tweets, and expand the vocabulary\n",
    "3. Combine similar emojis, such as those with hearts, into one category\n",
    "4. Expand beyond basic bag of words and linear least squares, which we will do in upcoming lessons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum: Ridge Regression\n",
    "\n",
    "In practice we want to implement a slight variant of this called \"Ridge Regression\", which will help avoid numerical problems and also help regularize the model so that the weights don't get too large. All we need to do is add a diagonal matrix filled with the \"ridge\" value that we will call $\\lambda$:\n",
    "\n",
    "$$ \\mathbf{\\beta} = ( \\mathbf{X}^T \\mathbf{X} + \\lambda I )^{-1} \\mathbf{X}^T \\mathbf{y} $$\n",
    "\n",
    "where $I$ is the identity matrix. Note that this also regularizes the bias term which is not always desirable, but we will leave the description this way for simplicity.\n",
    "\n",
    "For extra credit, try implementing Ridge Regression!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
